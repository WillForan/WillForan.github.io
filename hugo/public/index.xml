<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Foran</title>
    <link>/</link>
    <description>Recent content on Foran</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hugo w/Org Mode</title>
      <link>/hugo/</link>
      <pubDate>Sat, 13 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/hugo/</guid>
      <description>I&amp;#39;m transitioning to hugo from the custom python and org-export scripts tangled in org-mode /readme! I want to remove barriers to getting text out 1, streamline exported org style control, and get better RSS support.
Initially, the reports and publishing system itself were an effort to experiment with literate programming. I couldn&amp;#39;t find publishing tools that easily also executed babel and tangled files.
But my literate programming effort has been complicated.</description>
    </item>
    
    <item>
      <title>chatGPT inverted singularity</title>
      <link>/chatgpt/</link>
      <pubDate>Tue, 06 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/chatgpt/</guid>
      <description>60% of GPT-3s training is common crawl, an archive of text that makes up the internet. As people use large language models to add text to the internet, the models could end up with their own output as training input. Unchecked, the models will degrade: garbage in, garbage out. Instead of artificial intelligence recursively improving itself in the positive-feedback loop leading to the singularity, the output of the internet-as-a-corpus trained AI will repeatedly amplify noise and produce worsening results.</description>
    </item>
    
    <item>
      <title>org-babel code alias</title>
      <link>/org-codealias/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/org-codealias/</guid>
      <description>Org-babel plotting requires additional source block arguments: :results graphics file on image R blocks and :results output file in py blocks. This is a little cumbersome, especially because I have a hard time remembering the exact incantation and might end up writing it 10s of times in a document.
One particularly ugly kludge is to create new src_block types as aliases (eg. python=&amp;gt; pyplot). assign those aliases distinct #PROPERty: header-args Headers #+PROPERTY: header-args:pyplot :cache yes :session :dir /ssh:host:/path :eval noexport :results output file :session *py* #+PROPERTY: header-args:Rplot :results graphics file :session *R* :cache yes Elisp Org files can modify the emacs environment by sourcing elisp.</description>
    </item>
    
    <item>
      <title>cgi-bin and interpreter startup time</title>
      <link>/cgi-startuptime/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/cgi-startuptime/</guid>
      <description>Numbers Motivated by generating an intuition for the bare minimum time a cgi-bin script request could take, I&amp;#39;ve generated an unprincipled list of startup times for various interpreters.
There are lots of reasons to find this metric useless. It&amp;#39;s nothing like a measure of how fast code can run in the interpreter (arithmetic, regexp matching, etc), or even the time needed to load in common dependencies.
Eye-balling start up times for what I have installed on my system, there are 3 groups.</description>
    </item>
    
    <item>
      <title>Risk by email stats</title>
      <link>/risk/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/risk/</guid>
      <description>What Jump to *Stats. Log Copy/pasted from gamesbyemail.com&amp;#39;s game log. The page does fetch malformated json to populate the log, but the objects are dense. The presented log seems easier to parse. The interesting bits to extract are &amp;#34;Attacks&amp;#34;, &amp;#34;Defends&amp;#34;, &amp;#34;XXX defeated&amp;#34;, and &amp;#34;END OF TURN&amp;#34;.
Extract &amp;#34;Quick&amp;#34; perl code to extract
use strict; use experimental &amp;#39;switch&amp;#39;; open my $fh, &amp;#34;&amp;lt;&amp;#34;, &amp;#34;txt/risk_log.txt&amp;#34; or die &amp;#34;no file&amp;#34;; my @players=qw/Andrew Josh Will Ben Jim/; my $turn=0; my @res=(); my $init = {}; my $adjust= 0; my %player_contries=(); # track ownership my @keys = qw/turn a.</description>
    </item>
    
    <item>
      <title>Gopher blog</title>
      <link>/gopher/</link>
      <pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/gopher/</guid>
      <description>Gopher server I&amp;#39;ve added a gopher server to mirror content. I can not image it will be used seriously, but it seems important.
Gopher links to raw org text files. github.io gets prettified html.
Implementation My first attempt was with https://github.com/puckipedia/pyGopher. I didn&amp;#39;t spend enough time digging for documentation. Much later, I found geomyidae linked from suckless tools. By timing coincidence or by man page, it clicked.
git clone git://r-36.</description>
    </item>
    
    <item>
      <title>Netflix Usage</title>
      <link>/netflix/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/netflix/</guid>
      <description>Viewing Activity The &amp;#34;viewing activity&amp;#34; web interface is parsing json; no scraping needed!
The actual data returned incluces a lot of useful information too. Though it&amp;#39;s not displayed, we have time-stamped view time, series title, and duration.
It appears as though duration is not view duration but the duration of the episode/movie. I think there is not a good way to see if something was only partially watched. This is inflating summary metrics.</description>
    </item>
    
    <item>
      <title>Strava Tracked Workouts</title>
      <link>/strava/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/strava/</guid>
      <description>Viewing Activity pulling data Load https://www.strava.com/athlete/training and copy the requestion curl when inspecting network. Code below does not have the cookie -H param.
[ ! -d strava ] &amp;amp;&amp;amp; mkdir strava for i in {1..20}; do # copy from browser request. missing cookie here curl &amp;#34;https://www.strava.com/athlete/training_activities?keywords=&amp;amp;activity_type=&amp;amp;workout_type=&amp;amp;commute=&amp;amp;private_activities=&amp;amp;trainer=&amp;amp;gear=&amp;amp;new_activity_only=false&amp;amp;page=$i&amp;amp;per_page=20&amp;#34; &amp;gt; $i.json done load data library(jsonlite) library(lubridate) library(dplyr) library(ggplot2) library(cowplot) nasum &amp;lt;- function(x) sum(na.omit(as.numeric(x))) dlist &amp;lt;- lapply(Sys.glob(&amp;#39;strava/*json&amp;#39;), function(f) { fromJSON(f)$models %&amp;gt;% select(id,type,start_time, epoch=start_date_local_raw, distance,moving_time,elapsed_time, elevation_gain,calories ) }) d &amp;lt;- Reduce(rbind,dlist) d.</description>
    </item>
    
    <item>
      <title>Climbing Wall Route Annotation SPA</title>
      <link>/climbingwallspa/</link>
      <pubDate>Sun, 19 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/climbingwallspa/</guid>
      <description>What The Climbing Wall (TCW) has routes densily packed through different areas. To track our effort, we 1) identify the route and 2) annotate our progress.
identify Some routes have names. Most routes have the setter&amp;#39;s initials. All routes have a color and grading (VB-V9). Colors are repeated in different areas and occasionally within in an area, but the combined area, color, and difficulty should uniquely identify each route.</description>
    </item>
    
    <item>
      <title>readme: static generator python wrapping org-mode</title>
      <link>/readme/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/readme/</guid>
      <description>What This is a repo to generate html for github pages from org mode files. The file you are reading is both the readme and an individual &amp;#34;report&amp;#34; (linked into reports/) accessible from the generated index.
Motivation Org-mode babel is the coolest thing! I have a lot of git repos for one off experiments. I don&amp;#39;t know how to make literate programming work for me If I make it easy to export org-babel, I can use it to reduce friction on one off projects and test out ways to implement literate programming</description>
    </item>
    
  </channel>
</rss>
